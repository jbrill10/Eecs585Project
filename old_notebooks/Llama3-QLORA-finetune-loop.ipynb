{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839b1c07-2369-40ed-97b9-a5a961e32e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (0.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: accelerate in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (1.2.0.dev0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from accelerate) (2.1.3)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from accelerate) (0.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2024.7.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m venv env\n",
    "!source env/bin/activate\n",
    "!pip install sentencepiece\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets matplotlib einops\n",
    "!pip install accelerate\n",
    "!pip install -q wandb -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e97b4a6-89f6-4d5e-88f2-b4072d6183b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDas Ausführen von Zellen mit \"env (Python 3.11.6)\" erfordert das Paket ipykernel.\n",
      "\u001b[1;31mFühren Sie den folgenden Befehl aus, um „ipykernel“ in der Python-Umgebung zu installieren. \n",
      "\u001b[1;31mBefehl: '/Users/jeffb/school/eecs485Code/group_generator/env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!source env/bin/activate\n",
    "!export CUDA_VISIBLE_DEVICES=0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f7616c8-fbeb-4bea-85c8-d4a39162ab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from dataset import PlatypusDataset, track_performance\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from accelerate import Accelerator\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import wandb, os\n",
    "import transformers\n",
    "from datetime import datetime\n",
    "from accelerate import PartialState\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use only one GPU\n",
    "\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f39ec1a0-c645-4844-a90a-63ca75390652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"### Question: {example['instruction']}\\n ### Answer: {example['output']}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7abcbeaa-0593-4e44-8341-ee45f4eeb489",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@track_performance\n",
    "def finetune_new_model_on_dataset(train_dataset, val_dataset, base_model_id, base_model, source_name):    \n",
    "    model = prepare_model_for_kbit_training(base_model)\n",
    "    model = get_peft_model(model, config)\n",
    "    # print_trainable_parameters(model)\n",
    "    \n",
    "    # Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "    model = accelerator.prepare_model(model)\n",
    "\n",
    "    if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "        model.is_parallelizable = True\n",
    "        model.model_parallel = True\n",
    "\n",
    "    project = \"llama-finetune/ablation/\"\n",
    "    base_model_name = f\"meta-llama/{base_model_id}\"\n",
    "    run_name = base_model_name + \"-\" + project + source_name\n",
    "    output_dir = \"./\" + run_name\n",
    "    \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_val_dataset,\n",
    "        args=transformers.TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            warmup_steps=5,\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_checkpointing=True,\n",
    "            gradient_accumulation_steps=4,\n",
    "            max_steps=1000,\n",
    "            learning_rate=2.5e-5,\n",
    "            logging_steps=50,\n",
    "            bf16=True,\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "            logging_dir=\"./logs\",        # Directory for storing logs\n",
    "            save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "            save_steps=25,                # Save checkpoints every 50 steps\n",
    "            evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "            eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
    "            do_eval=True,                # Perform evaluation at the end of training\n",
    "            report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "            run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M'),}\",\n",
    "            local_rank=-1,\n",
    "            ddp_find_unused_parameters=False,\n",
    "            no_cuda=False,\n",
    "            dataloader_pin_memory=False\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "    trainer.train()\n",
    "\n",
    "    # base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     base_model_id,\n",
    "    #     quantization_config=bnb_config)\n",
    "    \n",
    "    # eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    #     base_model_id,\n",
    "    #     add_bos_token=True,\n",
    "    #     trust_remote_code=True,\n",
    "    # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9c5e8e7-4327-4995-93ad-03e47665f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "# plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e575ab-cf73-4817-b2ed-d49f4895cf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "wandb_project = \"llama-finetune\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "    \n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "max_length = 2048 # This was an appropriate max length for my dataset\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "    \n",
    "# base_model_id = \"meta-llama/Llama-3.1-8B\"\n",
    "base_model_id = \"HuggingFaceH4/tiny-random-LlamaForCausalLM\"\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map={\"\": \"cuda:0\"},  # Force everything to cuda:0\n",
    "    torch_dtype=torch.float16 )\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    "    use_fast=False, # needed for now, should be fixed soon\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "dataset = PlatypusDataset()\n",
    "data_sources = ['leetcode_ne', 'ARB', 'tigerbot-kaggle', 'MATH/PRM-800K', 'theoremqa', 'scienceqa', 'reclor', 'guanaco', 'scibench', 'airoboros']\n",
    "for source in data_sources:\n",
    "    print(f\"source: {source}\")\n",
    "    train_dataset = dataset.get_train_data_without_source(source)\n",
    "    val_dataset = dataset.get_val_data_without_source(source)\n",
    "    # Modify data preparation\n",
    "    tokenized_train_dataset = [\n",
    "        {k: v.to(accelerator.device) for k, v in generate_and_tokenize_prompt2(point).items()}\n",
    "        for point in train_dataset\n",
    "    ]\n",
    "    tokenized_val_dataset = [\n",
    "        {k: v.to(accelerator.device) for k, v in generate_and_tokenize_prompt2(point).items()}\n",
    "        for point in val_dataset\n",
    "    ]\n",
    "    print(f\"Training set size: {len(train_dataset)}\")\n",
    "    print(f\"Validation set size: {len(val_dataset)}\")\n",
    "    # plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)\n",
    "    finetune_new_model_on_dataset(train_dataset, val_dataset, base_model_id, model, accelerator, source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c877fe79-0f71-4b94-b09b-770f9c4b6811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_val_split = ds.train_test_split(test_size=0.2)\n",
    "\n",
    "# Extract the training and validation datasets\n",
    "# train_dataset = train_val_split['train']\n",
    "# val_dataset = train_val_split['test']\n",
    "\n",
    "# Print the size of the datasets\n",
    "# print(f\"Training set size: {len(train_dataset)}\")\n",
    "# print(f\"Validation set size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072d7438-2463-45d0-a966-88164372d95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(formatting_func(ds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f66eacb-57ac-491f-9a92-781df1cbea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, DataCollatorForLanguageModeling\n",
    "# from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d925a1-826b-4770-b927-6d5d2959c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model_id = \"meta-llama/Llama-3.1-8B\"\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e7b922-4247-4a61-9884-9131c6acfb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     base_model_id,\n",
    "#     padding_side=\"left\",\n",
    "#     add_eos_token=True,\n",
    "#     add_bos_token=True,\n",
    "#     use_fast=False, # needed for now, should be fixed soon\n",
    "# )\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a62421-7e3e-43df-8f06-e1c895644579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = 2048 # This was an appropriate max length for my dataset\n",
    "\n",
    "# def generate_and_tokenize_prompt2(prompt):\n",
    "#     result = tokenizer(\n",
    "#         formatting_func(prompt),\n",
    "#         truncation=True,\n",
    "#         max_length=max_length,\n",
    "#         padding=\"max_length\",\n",
    "#     )\n",
    "#     result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8538973d-1c01-48ce-93fe-fb071b0f52be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "# tokenized_val_dataset = val_dataset.map(generate_and_tokenize_prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad649024-998b-405c-b217-5bcb21ceb56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import prepare_model_for_kbit_training\n",
    "\n",
    "# model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dbd52b-2f0e-48db-99ba-b9b2a560d422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87df72ae-24fc-45e9-91af-9d4e51cfb626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c1f97-a255-4d68-adb0-498b8f55040a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f5258e-5ed6-4a81-bffd-ce238fa73b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from accelerate import Accelerator\n",
    "\n",
    "# accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1221595-3f5c-4a0c-a215-e75f822571b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# config = LoraConfig(\n",
    "#     r=8,\n",
    "#     lora_alpha=16,\n",
    "#     target_modules=[\n",
    "#         \"q_proj\",\n",
    "#         \"k_proj\",\n",
    "#         \"v_proj\",\n",
    "#         \"o_proj\",\n",
    "#         \"gate_proj\",\n",
    "#         \"up_proj\",\n",
    "#         \"down_proj\",\n",
    "#         \"lm_head\",\n",
    "#     ],\n",
    "#     bias=\"none\",\n",
    "#     lora_dropout=0.05,  # Conventional\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "# model = get_peft_model(model, config)\n",
    "# print_trainable_parameters(model)\n",
    "\n",
    "# # Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "# model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b92005-6029-4796-ae07-0db6ea5934ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c559f449-d619-415c-b3b3-fb7dcc54e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q wandb -U\n",
    "\n",
    "# import wandb, os\n",
    "# wandb.login()\n",
    "\n",
    "# wandb_project = \"llama-finetune\"\n",
    "# if len(wandb_project) > 0:\n",
    "#     os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514a9ed3-2bcf-4c20-bdae-f0578c71c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "#     model.is_parallelizable = True\n",
    "#     model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71a5af-c0f0-4754-bae0-1fc99df966b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# from datetime import datetime\n",
    "\n",
    "# sources = [\"a\", \"b\", \"c\"]\n",
    "    \n",
    "# project = \"llama-finetune\"\n",
    "# base_model_name = \"meta-llama/ablation/Llama-3.1-8B\"\n",
    "# run_name = base_model_name + \"-\" + project + source\n",
    "# output_dir = \"./\" + run_name\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# trainer = transformers.Trainer(\n",
    "#     model=model,\n",
    "#     train_dataset=tokenized_train_dataset,\n",
    "#     eval_dataset=tokenized_val_dataset,\n",
    "#     args=transformers.TrainingArguments(\n",
    "#         output_dir=output_dir,\n",
    "#         warmup_steps=5,\n",
    "#         per_device_train_batch_size=2,\n",
    "#         gradient_checkpointing=True,\n",
    "#         gradient_accumulation_steps=4,\n",
    "#         max_steps=1000,\n",
    "#         learning_rate=2.5e-5,\n",
    "#         logging_steps=50,\n",
    "#         bf16=True,\n",
    "#         optim=\"paged_adamw_8bit\",\n",
    "#         logging_dir=\"./logs\",        # Directory for storing logs\n",
    "#         save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "#         save_steps=25,                # Save checkpoints every 50 steps\n",
    "#         evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "#         eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
    "#         do_eval=True,                # Perform evaluation at the end of training\n",
    "#         report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "#         run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "#     ),\n",
    "#     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "# )\n",
    "\n",
    "# model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63256631-7d11-4d41-8a1a-f150055187ba",
   "metadata": {},
   "source": [
    "Reconnect the kernel after obtaining a good checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf435369-9495-4fc5-a202-0a1373772e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# base_model_id = \"meta-llama/Llama-3.1-8B\"\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#    base_model_id,\n",
    "#    quantization_config=bnb_config,  # Same quantization config as before\n",
    "#    device_map=\"auto\",\n",
    "#    trust_remote_code=True,\n",
    "#)\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     base_model_id,\n",
    "#     quantization_config=bnb_config)\n",
    "\n",
    "# eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     base_model_id,\n",
    "#     add_bos_token=True,\n",
    "#     trust_remote_code=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8223f762-4fe0-48e1-bf42-7f180ea802f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import PeftModel\n",
    "\n",
    "# ft_model = PeftModel.from_pretrained(base_model, \"meta-llama/Llama-3.1-8B-llama-finetune/checkpoint-675\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bac7189-d4b8-4a26-bb5f-74e916aebfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "# from deepeval.benchmarks import HellaSwag\n",
    "# from deepeval.benchmarks.tasks import HellaSwagTask\n",
    "# import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f2b1c1-d9b0-4c3d-9b6f-79a766ec5494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c887ee-ae8c-4fdc-9036-9ebb76248be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6721ab2-fbe8-4564-8743-efdb62735a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21265c7a-04eb-4473-b5d1-e169f2f6364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Llama3(DeepEvalBaseLLM):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model,\n",
    "#         tokenizer\n",
    "#     ):\n",
    "#         self.model = model\n",
    "#         self.tokenizer = tokenizer\n",
    "\n",
    "#     def load_model(self):\n",
    "#         return self.model\n",
    "\n",
    "#     def generate(self, prompt: str) -> str:\n",
    "#         sections = [section.strip() for section in prompt.split(\"\\n\\n\") if section.strip()]\n",
    "\n",
    "#         # Take the last section, including \"Answer:\" for context\n",
    "#         prompt = sections[-2]\n",
    "\n",
    "#         model = self.load_model()\n",
    "\n",
    "#         device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "#         model_inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "#         model.to(device)\n",
    "\n",
    "#         generated_ids = model.generate(\n",
    "#             **model_inputs, \n",
    "#             max_new_tokens=100, \n",
    "#             use_cache=True)\n",
    "        \n",
    "#         ans = self.tokenizer.batch_decode(generated_ids)[0]\n",
    "\n",
    "#         match = re.search(r\"Answer:\\s*([A-D])\", ans)\n",
    "\n",
    "#         if match:\n",
    "#             answer = match.group(1)\n",
    "#         else:\n",
    "#             answer = 'N/A'\n",
    "\n",
    "#         return answer\n",
    "\n",
    "#     async def a_generate(self, prompt: str) -> str:\n",
    "#         return self.generate(prompt)\n",
    "\n",
    "#     # This is optional.\n",
    "#     def batch_generate(self, promtps: list[str]) -> list[str]:\n",
    "#         model = self.load_model()\n",
    "#         device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "#         new_p = []\n",
    "#         for p in promtps:\n",
    "#             sections = [section.strip() for section in prompt.split(\"\\n\\n\") if section.strip()]\n",
    "#             new_p.append(sections[-2])\n",
    "            \n",
    "#         model_inputs = self.tokenizer(\n",
    "#             new_p,\n",
    "#             padding=True,    # Ensure equal-length inputs\n",
    "#             truncation=True, # Truncate inputs that exceed max_length\n",
    "#             max_length=512,\n",
    "#             return_tensors=\"pt\").to(device)\n",
    "#         model.to(device)\n",
    "\n",
    "#         generated_ids = model.generate(\n",
    "#             **model_inputs,\n",
    "#             max_new_tokens=100, use_cache=True)\n",
    "#         decoded_responses = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "#         res = []\n",
    "#         for ans in decoded_responses:\n",
    "#             match = re.search(r\"Answer:\\s*([A-D])\", ans)\n",
    "    \n",
    "#             if match:\n",
    "#                 res.append(match.group(1))\n",
    "#             else:\n",
    "#                 res.append('N/A')\n",
    "#         return res\n",
    "\n",
    "#     def get_model_name(self):\n",
    "#         return \"Llama 3\"\n",
    "\n",
    "\n",
    "# llama3 = Llama3(model=ft_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202934b5-e97b-4286-94cc-9b2333ebc96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark = HellaSwag(\n",
    "#     tasks=[HellaSwagTask.APPLYING_SUNSCREEN, HellaSwagTask.SKATEBOARDING],\n",
    "#     n_shots=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038fe6f2-13b6-4a9b-aa6f-8614d4df42d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = benchmark.evaluate(model=llama3, batch_size = 5)\n",
    "# print(\"Task-specific Scoress: \", benchmark.task_scores)\n",
    "# print(\"Detailed Predictions: \", benchmark.predictions)\n",
    "# print(benchmark.overall_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
