{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f8cacfa-abe9-4ba7-aacf-4a11b0fd3b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19bd5dc-7774-4cb0-b99c-b3c8d462dfc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7af4a0eb22d45af8a0139f878526dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14f503777474461af32fd5e36b1605a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6cdbc4d8884fe3b2c19da9168e22a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2ba037ff614e63a5f86adb3f1622f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476829d5624d4db8809d9e6beec306a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e320cfb1d00d47d188e29bdc31ba2421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5a54ed8d944f26ad4838fb28af6a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb8c14b0a7943aaa90061c8e944710d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661ff07e25ec425d82eb996106a2c237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80da5ca1cd1f493c87ebe4a4bac88b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d93dfee7af46099c834be83c174357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d170f459c944c35bd435743bfae6f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36516b5f-e4af-452e-9852-52870ebeaf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from deepeval.benchmarks import HellaSwag\n",
    "from deepeval.benchmarks.tasks import HellaSwagTask\n",
    "from deepeval.benchmarks import MMLU\n",
    "from deepeval.benchmarks.tasks import MMLUTask\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6acc5c0-9505-4a91-8123-f436464887fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n"
     ]
    }
   ],
   "source": [
    "class Llama3(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        sections = [section.strip() for section in prompt.split(\"\\n\\n\") if section.strip()]\n",
    "\n",
    "        # Take the last section, including \"Answer:\" for context\n",
    "        prompt = sections[-2]\n",
    "\n",
    "        model = self.load_model()\n",
    "\n",
    "        device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "        model_inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "        model.to(device)\n",
    "\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs, \n",
    "            max_new_tokens=100, \n",
    "            use_cache=True)\n",
    "        \n",
    "        ans = self.tokenizer.batch_decode(generated_ids)[0]\n",
    "\n",
    "        match = re.search(r\"Answer:\\s*([A-D])\", ans)\n",
    "\n",
    "        if match:\n",
    "            answer = match.group(1)\n",
    "        else:\n",
    "            answer = 'N/A'\n",
    "\n",
    "        return answer\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    # This is optional.\n",
    "    def batch_generate(self, promtps: list[str]) -> list[str]:\n",
    "        model = self.load_model()\n",
    "        device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "        new_p = []\n",
    "        for p in promtps:\n",
    "            sections = [section.strip() for section in prompt.split(\"\\n\\n\") if section.strip()]\n",
    "            new_p.append(sections[-2])\n",
    "            \n",
    "        model_inputs = self.tokenizer(\n",
    "            new_p,\n",
    "            padding=True,    # Ensure equal-length inputs\n",
    "            truncation=True, # Truncate inputs that exceed max_length\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\").to(device)\n",
    "        model.to(device)\n",
    "\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=100, use_cache=True)\n",
    "        decoded_responses = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        res = []\n",
    "        for ans in decoded_responses:\n",
    "            match = re.search(r\"Answer:\\s*([A-D])\", ans)\n",
    "    \n",
    "            if match:\n",
    "                res.append(match.group(1))\n",
    "            else:\n",
    "                res.append('N/A')\n",
    "        return res\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Llama 3\"\n",
    "\n",
    "\n",
    "llama3 = Llama3(model=model, tokenizer=tokenizer)\n",
    "\n",
    "test = \"\"\"\n",
    "\n",
    "The following are multiple choice questions (with answers) are sentence completion problems about Applying sunscreen.\n",
    "\n",
    "Then, the man writes over the snow covering the window of a car, and a woman wearing winter clothes smiles. then\n",
    "A., the man adds wax to the windshield and cuts it.\n",
    "B., a person board a ski lift, while two men supporting the head of the person wearing winter clothes snow as the we girls sled.\n",
    "C., the man puts on a christmas coat, knitted with netting.\n",
    "D., the man continues removing the snow on his car.\n",
    "Answer: D\n",
    "\n",
    "A female chef in white uniform shows a stack of baking pans in a large kitchen presenting them. the pans\n",
    "A. contain egg yolks and baking soda.\n",
    "B. are then sprinkled with brown sugar.\n",
    "C. are placed in a strainer on the counter.\n",
    "D. are filled with pastries and loaded into the oven.\n",
    "Answer: D\n",
    "\n",
    "The man in the center is demonstrating a hairstyle on the person wearing the blue shirt. the man in the blue shirt\n",
    "A. is standing on the sponge cutting the hair of the person wearing the blue shirt.\n",
    "B. is doing the hairstyle with his hand and the hairspray.\n",
    "C. sits on the chair next to the sink.\n",
    "D. is being shown eye to eye.\n",
    "Answer: C\n",
    "\n",
    "Two bodybuilder women are seated at a table. they\n",
    "A. are talking about diving techniques, bribing each other with muscle' n strength.\n",
    "B. are working out on exercise bikes.\n",
    "C. are arm wrestling, vieing to win.\n",
    "D. are shown on parallel bars.\n",
    "Answer: C\n",
    "\n",
    "This is a tutorial on how to start a campfire. it\n",
    "A. shows how to light the fire by rubbing a lid on it.\n",
    "B. is supposed to be a fire log, but your dad said that he might have burned it, and that if he catches fire it will hurt him.\n",
    "C. shows the campfire burning on the ground.\n",
    "D. is a green and red sweet and the recipe is to make it hot and then puts it in a pan to simmer.\n",
    "Answer: C\n",
    "\n",
    "A woman puts some lotion on her hand. She rubs the lotion onto her face. a cartoon demonstration\n",
    "A. is shown with a curling brush.\n",
    "B. is then shown of a woman crying.\n",
    "C. is shown on the screen.\n",
    "D. of a cat is shown.\n",
    "Answer:\n",
    "\n",
    "Output A, B, C, or D. Full answer not needed.\n",
    "\"\"\"\n",
    "\n",
    "# Call the generate method\n",
    "print(llama3.generate(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1e05f49-ec88-4cbc-a530-7d94c2ada56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = MMLU(\n",
    "    tasks=[MMLUTask.HIGH_SCHOOL_COMPUTER_SCIENCE, MMLUTask.ASTRONOMY],\n",
    "    n_shots=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51d734e1-c3a5-435b-97aa-8d22cb3ba199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06884c442044fb59ef21aa0391c05f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/28.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4041a3dc21d844ccb2da1d0448a12623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mmlu.py:   0%|          | 0.00/5.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75191c3d2544457f8115ccf8b7bc2575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431e2371e37a4b4288dd619cc1999519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b163f8da8df4161aa67acfab3e2d15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e04d5bedf9e4007af8fcf468fa36b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high_school_computer_science:   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:   1%|          | 1/100 [00:00<00:59,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:   3%|▎         | 3/100 [00:00<00:22,  4.30it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:   4%|▍         | 4/100 [00:00<00:19,  4.87it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:   6%|▌         | 6/100 [00:01<00:14,  6.63it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:   7%|▋         | 7/100 [00:03<01:11,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:   8%|▊         | 8/100 [00:03<00:54,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  10%|█         | 10/100 [00:03<00:33,  2.66it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  12%|█▏        | 12/100 [00:04<00:23,  3.73it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  13%|█▎        | 13/100 [00:04<00:21,  4.13it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  14%|█▍        | 14/100 [00:04<00:18,  4.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  16%|█▌        | 16/100 [00:04<00:13,  6.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  18%|█▊        | 18/100 [00:04<00:11,  6.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  19%|█▉        | 19/100 [00:04<00:11,  7.00it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  20%|██        | 20/100 [00:07<00:52,  1.54it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  21%|██        | 21/100 [00:07<00:41,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  22%|██▏       | 22/100 [00:09<01:14,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  23%|██▎       | 23/100 [00:10<01:07,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  24%|██▍       | 24/100 [00:12<01:36,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  25%|██▌       | 25/100 [00:12<01:10,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  27%|██▋       | 27/100 [00:12<00:40,  1.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  28%|██▊       | 28/100 [00:13<00:32,  2.21it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  29%|██▉       | 29/100 [00:13<00:26,  2.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  31%|███       | 31/100 [00:13<00:16,  4.08it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  33%|███▎      | 33/100 [00:13<00:12,  5.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  35%|███▌      | 35/100 [00:13<00:09,  6.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  37%|███▋      | 37/100 [00:13<00:07,  8.06it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  39%|███▉      | 39/100 [00:16<00:34,  1.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  41%|████      | 41/100 [00:17<00:25,  2.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  43%|████▎     | 43/100 [00:17<00:18,  3.07it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  45%|████▌     | 45/100 [00:17<00:13,  4.01it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  47%|████▋     | 47/100 [00:17<00:10,  5.10it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  49%|████▉     | 49/100 [00:17<00:08,  6.04it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  51%|█████     | 51/100 [00:20<00:23,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  53%|█████▎    | 53/100 [00:20<00:16,  2.86it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  55%|█████▌    | 55/100 [00:20<00:12,  3.74it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  57%|█████▋    | 57/100 [00:20<00:09,  4.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  59%|█████▉    | 59/100 [00:20<00:07,  5.19it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  61%|██████    | 61/100 [00:20<00:06,  6.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  63%|██████▎   | 63/100 [00:21<00:04,  7.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  65%|██████▌   | 65/100 [00:21<00:04,  8.32it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  67%|██████▋   | 67/100 [00:21<00:03,  9.34it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  69%|██████▉   | 69/100 [00:21<00:03,  9.63it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  71%|███████   | 71/100 [00:21<00:03,  9.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  73%|███████▎  | 73/100 [00:22<00:02,  9.77it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  75%|███████▌  | 75/100 [00:22<00:02,  8.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  76%|███████▌  | 76/100 [00:22<00:02,  8.70it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  78%|███████▊  | 78/100 [00:22<00:02,  8.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  80%|████████  | 80/100 [00:22<00:01, 10.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  82%|████████▏ | 82/100 [00:23<00:01, 10.05it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  84%|████████▍ | 84/100 [00:23<00:01, 10.17it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  86%|████████▌ | 86/100 [00:23<00:01, 10.58it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  88%|████████▊ | 88/100 [00:25<00:05,  2.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  89%|████████▉ | 89/100 [00:26<00:04,  2.31it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  91%|█████████ | 91/100 [00:28<00:05,  1.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  92%|█████████▏| 92/100 [00:28<00:04,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  94%|█████████▍| 94/100 [00:28<00:02,  2.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  96%|█████████▌| 96/100 [00:28<00:01,  3.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science:  98%|█████████▊| 98/100 [00:28<00:00,  4.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing high_school_computer_science: 100%|██████████| 100/100 [00:29<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=high_school_computer_science): 0.57\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9251ac88589a439999b82ac8f7e32f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3798fbc342349caa14ac365598979a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7441ee5646463799ad3bbd14e83425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing astronomy:   0%|          | 0/152 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:   1%|▏         | 2/152 [00:00<00:09, 15.36it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:   3%|▎         | 4/152 [00:00<00:11, 13.03it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:   4%|▍         | 6/152 [00:00<00:11, 13.16it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:   5%|▌         | 8/152 [00:00<00:11, 12.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:   7%|▋         | 10/152 [00:00<00:12, 11.63it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:   8%|▊         | 12/152 [00:00<00:12, 11.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:   9%|▉         | 14/152 [00:01<00:11, 12.10it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  11%|█         | 16/152 [00:01<00:10, 12.96it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  12%|█▏        | 18/152 [00:01<00:10, 12.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  13%|█▎        | 20/152 [00:01<00:10, 12.66it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  14%|█▍        | 22/152 [00:01<00:10, 12.81it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  16%|█▌        | 24/152 [00:01<00:10, 12.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  17%|█▋        | 26/152 [00:02<00:09, 12.64it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  18%|█▊        | 28/152 [00:02<00:09, 12.81it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  20%|█▉        | 30/152 [00:02<00:09, 12.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  21%|██        | 32/152 [00:02<00:10, 10.99it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  22%|██▏       | 34/152 [00:02<00:10, 11.15it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  24%|██▎       | 36/152 [00:03<00:10, 10.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  25%|██▌       | 38/152 [00:03<00:10, 11.22it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  26%|██▋       | 40/152 [00:03<00:10, 10.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  28%|██▊       | 42/152 [00:03<00:10, 10.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  29%|██▉       | 44/152 [00:03<00:10, 10.52it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  30%|███       | 46/152 [00:03<00:10, 10.36it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  32%|███▏      | 48/152 [00:04<00:09, 10.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  33%|███▎      | 50/152 [00:04<00:09, 10.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  34%|███▍      | 52/152 [00:04<00:09, 10.81it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  36%|███▌      | 54/152 [00:04<00:08, 11.36it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  37%|███▋      | 56/152 [00:04<00:08, 11.81it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  38%|███▊      | 58/152 [00:04<00:07, 12.20it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  39%|███▉      | 60/152 [00:05<00:15,  5.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  41%|████      | 62/152 [00:05<00:13,  6.87it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  42%|████▏     | 64/152 [00:06<00:10,  8.03it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  43%|████▎     | 66/152 [00:06<00:09,  9.09it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  45%|████▍     | 68/152 [00:06<00:08, 10.03it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  46%|████▌     | 70/152 [00:06<00:07, 10.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  47%|████▋     | 72/152 [00:06<00:07, 10.77it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  49%|████▊     | 74/152 [00:06<00:06, 11.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  50%|█████     | 76/152 [00:06<00:06, 11.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  51%|█████▏    | 78/152 [00:07<00:06, 12.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  53%|█████▎    | 80/152 [00:07<00:05, 12.54it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  54%|█████▍    | 82/152 [00:07<00:05, 12.17it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  55%|█████▌    | 84/152 [00:07<00:05, 11.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  57%|█████▋    | 86/152 [00:07<00:05, 12.26it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  58%|█████▊    | 88/152 [00:07<00:05, 12.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  59%|█████▉    | 90/152 [00:08<00:04, 12.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  61%|██████    | 92/152 [00:10<00:24,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  62%|██████▏   | 94/152 [00:10<00:17,  3.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  63%|██████▎   | 96/152 [00:10<00:13,  4.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  64%|██████▍   | 98/152 [00:10<00:10,  5.28it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  66%|██████▌   | 100/152 [00:11<00:08,  6.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  67%|██████▋   | 102/152 [00:11<00:06,  7.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  68%|██████▊   | 104/152 [00:11<00:05,  8.97it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  70%|██████▉   | 106/152 [00:11<00:08,  5.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  71%|███████   | 108/152 [00:12<00:06,  6.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  72%|███████▏  | 110/152 [00:12<00:05,  7.80it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  74%|███████▎  | 112/152 [00:12<00:04,  9.14it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  75%|███████▌  | 114/152 [00:12<00:03,  9.74it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  76%|███████▋  | 116/152 [00:12<00:03, 10.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  78%|███████▊  | 118/152 [00:12<00:03, 10.11it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  79%|███████▉  | 120/152 [00:13<00:03, 10.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  80%|████████  | 122/152 [00:13<00:02, 11.16it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  82%|████████▏ | 124/152 [00:13<00:02, 12.16it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  83%|████████▎ | 126/152 [00:13<00:02, 12.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  84%|████████▍ | 128/152 [00:13<00:01, 12.98it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  86%|████████▌ | 130/152 [00:14<00:03,  6.85it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  87%|████████▋ | 132/152 [00:14<00:02,  7.78it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  88%|████████▊ | 134/152 [00:14<00:02,  8.85it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  89%|████████▉ | 136/152 [00:14<00:01,  9.82it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  91%|█████████ | 138/152 [00:15<00:02,  5.04it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  92%|█████████▏| 140/152 [00:15<00:01,  6.31it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  93%|█████████▎| 142/152 [00:15<00:01,  7.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  95%|█████████▍| 144/152 [00:16<00:00,  8.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  96%|█████████▌| 146/152 [00:16<00:00,  9.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  97%|█████████▋| 148/152 [00:16<00:00, 11.07it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy:  99%|█████████▊| 150/152 [00:18<00:00,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing astronomy: 100%|██████████| 152/152 [00:18<00:00,  8.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=astronomy): 0.6644736842105263\n",
      "Overall MMLU Accuracy: 0.626984126984127\n",
      "Task-specific Scoress:                             Task     Score\n",
      "0  high_school_computer_science  0.570000\n",
      "1                     astronomy  0.664474\n",
      "Detailed Predictions:                               Task  \\\n",
      "0    high_school_computer_science   \n",
      "1    high_school_computer_science   \n",
      "2    high_school_computer_science   \n",
      "3    high_school_computer_science   \n",
      "4    high_school_computer_science   \n",
      "..                            ...   \n",
      "247                     astronomy   \n",
      "248                     astronomy   \n",
      "249                     astronomy   \n",
      "250                     astronomy   \n",
      "251                     astronomy   \n",
      "\n",
      "                                                 Input Prediction  Correct  \n",
      "0    Let x = 1. What is x << 3 in Python 3?\\nA. 1\\n...          D        0  \n",
      "1    In Python 3, which of the following function c...          A        1  \n",
      "2    A user enters a Web address in a browser, and ...          A        1  \n",
      "3    Digital images are often represented by the re...          B        0  \n",
      "4    A programmer is writing a program that is inte...          B        1  \n",
      "..                                                 ...        ...      ...  \n",
      "247  Find the best approximation for the surface te...          D        0  \n",
      "248  Previous IAAC rounds featured Proxima/Alpha Ce...          A        0  \n",
      "249  How are planetary rings made?\\nA. From the dis...          A        1  \n",
      "250  The lunar maria are:\\nA. ancient heavily crate...          C        1  \n",
      "251  One astronomical unit (AU) is equal to approxi...          C        0  \n",
      "\n",
      "[252 rows x 4 columns]\n",
      "0.626984126984127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = benchmark.evaluate(model=llama3, batch_size = 5)\n",
    "print(\"Task-specific Scoress: \", benchmark.task_scores)\n",
    "print(\"Detailed Predictions: \", benchmark.predictions)\n",
    "print(benchmark.overall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab24c010-16a1-45c9-8ae7-e4148c01db09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Python 3, which of the following function convert a string to an int in python?\n",
      "A. int(x [,base])\n",
      "B. long(x [,base] )\n",
      "C. float(x)\n",
      "D. str(x)\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(benchmark.predictions['Input'].iloc[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
